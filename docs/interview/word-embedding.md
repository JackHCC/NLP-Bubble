# Word Embedding

## Q&A

1. 有没有使用自己的数据训练过Word2vec，详细说一下过程。包括但是不限于：语料如何获取，清理以及语料的大小，超参数的选择及其原因，词表以及维度大小，训练时长等等细节点。

2. Word2vec模型是如何获得词向量的？聊一聊你对词嵌入的理解？如何理解分布式假设？

3. **如何评估训练出来的词向量的好坏？**

   - **内在评估方法**，直接评估词语之间的相似性

   - - - 相关性: 对两个词之间的相关性进行人工评分。两个词之间的cos相似度作为基于词向量的评分。通过比较cos相似度和人工评分的相关性，来评估。
       - 类比analogy： vec(中国)-vec(北京)=vec(法国)-vec(巴黎)
       - 分类：对词打上类别标签，通过词向量来聚类，评判聚类好坏
       - 词法：确定一个名词是主语还是宾语

   - **外在评估方法**，通过下游任务的表现来间接评估

4. **Word2vec模型如何做到增量训练？**

   - 参考：https://blog.csdn.net/qq_43404784/article/details/83794296

5. 大致聊一下 **word2vec这个模型的细节**，包括但不限于：两种模型以及两种优化方法（大致聊一下就可以，下面会详细问）

   - CBOW，skip-gram
   - 优化：
     - 词组代替单词
     - 高频词减少采样
     - 负采样（Negative Sampling）与层序Sortmax（霍夫曼算法）

6. **解释一下 hierarchical softmax 的流程**(CBOW and Skip-gram)

   - 参考：https://blog.csdn.net/imsuhxz/article/details/82115681

7. 基于6，可以展开问一下模型如何获取输入层，有没有隐层，输出层是什么情况。

8. 基于6，可以展开问输出层为何选择霍夫曼树，它有什么优点，为何不选择其他的二叉树

9. 基于6，可以问该模型的复杂度是多少，目标函数分别是什么，如何做到更新梯度（尤其是如何更新输入向量的梯度）

10. 基于6，可以展开问一下 hierarchical softmax 这个模型 有什么缺点

11. **聊一下负采样模型优点（为什么使用负采样技术）？**

    - 加快训练速度

12. **如何对输入进行负采样（负采样的具体实施细节是什么）？**

    - 创建两个线段，第一个线段切开词表大小的份数，每个份数的**长度和频率正比**。
    - 第二个线段均分M个，然后随机取整数，整数落在第二个线段那里，然后取第一个线段对应的词，如果碰到是自己，那么就跳过。

    ![](../../image/img/interview/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20201224211840.png)

13. 负采样模型对应的目标函数分别是什么（CBOW and Skip-gram）

14. CBOW和skip-gram相较而言，彼此相对适合哪些场景？

    - 参考：https://github.com/km1994/NLP-Interview-Notes/tree/main/NLPinterview/PreTraining/word2vec

15. 有没有使用Word2vec计算过句子的相似度，效果如何，有什么细节可以分享出来

16. 详细聊一下Glove细节，它是如何进行训练的？有什么优点？什么场景下适合使用？与Word2vec相比，有什么区别（比如损失函数）？

17. 详细聊一下Fasttext细节，每一层都代表了什么？它与Wod2vec的区别在哪里？什么情况下适合使用Fasttext这个模型？

18. ELMO的原理是什么？以及它的两个阶段分别如何应用？（第一阶段如何预训练，第二阶段如何在下游任务使用）

19. ELMO的损失函数是什么？它是一个双向语言模型吗？为什么？

20. ELMO的优缺点分别是什么？为什么可以做到一词多义的效果？
